# Batch Inference

> <- Back to [[Model Serving]]

Running predictions offline on batches of data, typically on a schedule. Results stored in a database or cache for serving. Low latency at serving time but predictions may be stale.

## Related

- [[Real-Time Inference]] (online alternative)
- [[Pipeline Orchestration]] (schedules batch jobs)

---

#mlops #inference #batch
