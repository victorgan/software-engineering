# Bidirectional RNNs

> <- Back to [[Recurrent Neural Networks]]

Process the sequence in both forward and backward directions, concatenating the hidden states. Captures context from both past and future for each position. Used in NER, sentiment analysis, and speech recognition.

## Related

- [[LSTM]] (commonly used bidirectionally)
- [[Encoder-Only Models]] (BERT achieves bidirectionality via masking)

---

#deep-learning #rnn #bidirectional
