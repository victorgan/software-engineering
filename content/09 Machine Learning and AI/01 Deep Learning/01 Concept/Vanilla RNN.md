# Vanilla RNN

> <- Back to [[Recurrent Neural Networks]]

The simplest recurrent architecture. Processes sequences one step at a time, maintaining a hidden state that captures information from previous steps. Suffers from the vanishing gradient problem, making it difficult to learn long-range dependencies.

## Related

- [[LSTM]] (solves vanishing gradient problem)
- [[GRU]] (simplified alternative to LSTM)

---

#deep-learning #rnn
