# Classification Metrics

> <- Back to [[Model Evaluation]]

Metrics for evaluating models that predict discrete categories. Different metrics emphasize different aspects of performance; the right choice depends on the problem and business context.

## Key Metrics

- **Accuracy** -- fraction of correct predictions (misleading for imbalanced data)
- **Precision** -- of predicted positives, how many are correct
- **Recall** -- of actual positives, how many were found
- **F1-Score** -- harmonic mean of precision and recall
- **AUC-ROC** -- area under receiver operating characteristic curve
- **Confusion Matrix** -- full breakdown of TP, FP, TN, FN

## Related

- [[Regression Metrics]] (continuous outputs)
- [[Bias-Variance Tradeoff]] (precision-recall tradeoff)

---

#ml #evaluation #classification #metrics
